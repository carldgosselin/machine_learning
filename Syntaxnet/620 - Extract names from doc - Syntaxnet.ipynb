{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract names from doc - Syntaxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adjust scroll bar activation threshold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 100;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agreements data read successfully!\n",
      "Selected text file name from csv ->  Cloudfind Limited_eula.txt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose agreement file from list in csv\n",
    "\n",
    "# read csv file\n",
    "agreements_data = pd.read_csv(\"data/agreements_dataset1.csv\")\n",
    "print \"\\n\",\"Agreements data read successfully!\"\n",
    "\n",
    "# Select text file name.  4 is a simple sentence\n",
    "selected_text_file_name = agreements_data['file_name'][19]\n",
    "print \"Selected text file name from csv -> \", selected_text_file_name, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agreement text has been cleansed and parsed into separate sentences. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleanse and parse text into separate sentences\n",
    "\n",
    "caps = \"([A-Z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr|www|1|2|2|3|4|5|6|7|8|9|10)[.]\"\n",
    "suffixes = \"(Inc|INC|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"\\r\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "filepath = \"data/\" + selected_text_file_name\n",
    "with open(filepath, 'r') as my_agreement:\n",
    "    my_agreement_text=my_agreement.read()\n",
    "\n",
    "my_agreement_lines = split_into_sentences(my_agreement_text)\n",
    "print \"\\n\", \"Agreement text has been cleansed and parsed into separate sentences.\", \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of sentences retrieved with the word 'between': 2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find sentences with \"between\"\n",
    "\n",
    "key_sentences = []\n",
    "type_of_sentence = \"\"\n",
    "key_words = [\"between\"]\n",
    "Party1 = \"not found yet\"\n",
    "Party2 = \"not found yet\"\n",
    "\n",
    "for line in my_agreement_lines:\n",
    "    for word in key_words:\n",
    "        if word in line.lower(): \n",
    "            key_sentences.append(line)\n",
    "                \n",
    "print \"\\n\", \"Number of sentences retrieved with the word 'between':\", len(key_sentences),  \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences retrieved: 2\n",
      "Processing the first retrieved sentence\n"
     ]
    }
   ],
   "source": [
    "# Find key sentence(s)\n",
    "\n",
    "def pre_process_sentence(key_sentences):\n",
    "\n",
    "    with open(\"data/syntaxnet_processing.txt\", \"w\") as text_file:\n",
    "        text_file.write(key_sentences[0])\n",
    "\n",
    "if len(key_sentences) == 0: \n",
    "    print \"There is no 'between' clause in the document.\"\n",
    "    print \"Party1 ->\",Party1\n",
    "    print \"Party2 ->\",Party2\n",
    "    \n",
    "if len(key_sentences) == 1: pre_process_sentence(key_sentences) \n",
    "    \n",
    "if len(key_sentences) > 1: \n",
    "    \n",
    "    key_sentences2 = []\n",
    "    listOfWords = ['between','agreement']\n",
    "\n",
    "    for line in key_sentences:\n",
    "        if all(word in line.lower() for word in listOfWords):\n",
    "            key_sentences2.append(line)\n",
    "    \n",
    "    if len(key_sentences2) == 0: print \"no key sentences returned.\" \n",
    "    if len(key_sentences2) == 1: pre_process_sentence(key_sentences2)\n",
    "    if len(key_sentences2) > 1:\n",
    "        print \"Number of sentences retrieved:\", len(key_sentences2)\n",
    "        print \"Processing the first retrieved sentence\"\n",
    "        pre_process_sentence(key_sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "syntaxnet/demo_carl2.sh < data/syntaxnet_processing.txt > data/syntaxnet_processing_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read from Syntaxnet output tab delimited textfile\n",
    "\n",
    "syntaxnet_array = []\n",
    "\n",
    "def readata(filename):\n",
    "      file=open(filename,'r')\n",
    "      lines=file.readlines()\n",
    "      lines=lines[:-1]\n",
    "      sentence_data_syntaxnet=csv.reader(lines,delimiter='\\t')\n",
    "      syntaxnet_array=list(sentence_data_syntaxnet)\n",
    "      return (syntaxnet_array)\n",
    "                    \n",
    "syntaxnet_array = readata(\"data/syntaxnet_processing_output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert text file to CSV (for debugging purposes)\n",
    "\n",
    "txt_file = r\"data/syntaxnet_processing_output.txt\"\n",
    "csv_file = r\"data/syntaxnet_processing_output.csv\"\n",
    "\n",
    "in_txt = csv.reader(open(txt_file, \"rb\"), delimiter = '\\t')\n",
    "out_csv = csv.writer(open(csv_file, 'wb'))\n",
    "\n",
    "out_csv.writerows(in_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agreement: Cloudfind Limited_eula.txt\n",
      "Party1: you , Party2: Cloudfind Limited \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find party1 and party2\n",
    "\n",
    "# Print results\n",
    "def extract_result(Party1, Party2):\n",
    "    print \"\\n\", \"Agreement:\", selected_text_file_name\n",
    "    print \"Party1:\",Party1,\",\",\"Party2:\",Party2,\"\\n\"\n",
    " \n",
    "def process_sentence(syntaxnet_array):\n",
    "    \n",
    "    # Locate \"between\"\n",
    "    array_count = 0\n",
    "    for i in syntaxnet_array:\n",
    "        array_count += 1\n",
    "        if i[1].lower() == \"between\": break\n",
    "    \n",
    "    # Capture word after 'between'\n",
    "    Party1 = syntaxnet_array[array_count][1]\n",
    "        \n",
    "    # Check for companies with multiple words\n",
    "    array_count += 1\n",
    "    while syntaxnet_array[array_count][3] == \"NOUN\":\n",
    "        Party1 = Party1 + \" \" + syntaxnet_array[array_count][1]\n",
    "        array_count += 1\n",
    "        \n",
    "    # Locate \"and\"\n",
    "    array_count2 = array_count\n",
    "    for i in syntaxnet_array:\n",
    "        if syntaxnet_array[array_count2][1].lower() == \"and\": \n",
    "            array_count2 += 1\n",
    "            break\n",
    "        array_count2 += 1\n",
    "    \n",
    "    # Capture word after 'between'\n",
    "    Party2 = syntaxnet_array[array_count2][1]\n",
    "        \n",
    "    # Check for companies with multiple words\n",
    "    array_count2 += 1\n",
    "    while syntaxnet_array[array_count2][3] == \"NOUN\":\n",
    "        Party2 = Party2 + \" \" + syntaxnet_array[array_count2][1]\n",
    "        array_count2 += 1\n",
    "    \n",
    "    print \"\\n\", \"Agreement:\", selected_text_file_name\n",
    "    print \"Party1:\",Party1,\",\",\"Party2:\",Party2,\"\\n\"\n",
    "    \n",
    "process_sentence(syntaxnet_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous results\n",
    "\n",
    "**Agreement**: Google_Construction_Agreement_C.txt <br>\n",
    "**Party1**: GOOGLE INC. , **Party2**: S.G. Cunningham \n",
    "\n",
    "**Agreement**: 2Think1 Solutions Inc_eula.txt <br>\n",
    "**Party1**: you , **Party2**: 2THINK1 SOLUTIONS INC\n",
    "\n",
    "**Agreement**: ABBYY_eula.txt <br>\n",
    "**Party1**: you , **Party2**: ABBYY \n",
    "\n",
    "**Agreement**: Aeria Games & Entertainment Inc_eula.txt <br>\n",
    "**Party1**: YOU , **Party2**: AGE \n",
    "\n",
    "**Agreement**: AllCursors_eula.txt <br>\n",
    "**Party1**: you , **Party2**: Licensor \n",
    "\n",
    "**Agreement**: ALM Works Ltd_eula.txt **(FAIL)** <br>\n",
    "**Party1**: you , **Party2**: the \n",
    "\n",
    "**Agreement**: AnyChart_eula.txt <br>\n",
    "**Party1**: you , **Party2**: Sibental \n",
    "\n",
    "**Agreement**: AOL Inc_eula.txt <br>\n",
    "**Party1**: you , **Party2**: us \n",
    "\n",
    "**Agreement**: app square OG_eula.txt <br>\n",
    "**Party1**: you , **Party2**: appsquare OG\n",
    "\n",
    "**Agreement**: APPEARTOME LIMITED_eula.txt <br>\n",
    "**Party1**: you , **Party2**: Appeartome Limited \n",
    "\n",
    "**Agreement**: Atlassian_eula.txt <br>\n",
    "**Party1**: you , **Party2**: Atlassian Pty Ltd \n",
    "\n",
    "**Agreement**: Avanquest Software SA_eula.txt <br>\n",
    "**Party1**: you , **Party2**: Avanquest Software S.A. \n",
    "\n",
    "**Agreement**: Bitdefender_eula.txt <br>\n",
    "**Party1**: you , **Party2**: BITDEFENDER \n",
    "\n",
    "**Agreement**: Blizzard Entertainment Inc_eula.txt **(FAIL)** <br>\n",
    "**Party1**: the parties , **Party2**: supersedes\n",
    "\n",
    "**Agreement**: BTC_eula.txt <br>\n",
    "**Party1**: you , **Party2**: bigtincan \n",
    "\n",
    "**Agreement**: Caphyon_eula.txt <br>\n",
    "**Party1**: YOU , **Party2**: CAPHYON \n",
    "\n",
    "**Agreement**: CareerBuilder_eula.txt <br>\n",
    "**Party1**: you , **Party2**: CareerBuilder \n",
    "\n",
    "**Agreement**: Caristix_eula.txt <br>\n",
    "**Party1**: you , **Party2**: Caristix \n",
    "\n",
    "**Agreement**: ChemAxon Ltd_eula.txt <br>\n",
    "**Party1**: you , **Party2**: ChemAxon Ltd.\n",
    "\n",
    "**Agreement**: Cloudfind Limited_eula.txt <br>\n",
    "**Party1**: you , **Party2**: Cloudfind Limited\n",
    "\n",
    "**Agreement**: Concurrent Inc_eula.txt <br>\n",
    "**Party1**: Concurrent , **Party2**: you "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
